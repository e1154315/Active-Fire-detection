{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add dice coefficient and iou\n",
    "# use Adadelta as optimizer\n",
    "# use monitor='val_dice_coefficient' when training \n",
    "## use monitor='val_dice_coefficient' when training may have a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61d0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "#metrics\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-6  # 为了避免除以零\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=[1,2,3])\n",
    "    iou = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb1de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate\n",
    "\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Dice loss, a measure of overlap between two samples.\n",
    "    \"\"\"\n",
    "    smooth = 1.0  # Smooth factor to avoid division by zero\n",
    "    y_true_f = keras.flatten(y_true)\n",
    "    y_pred_f = keras.flatten(y_pred)\n",
    "    intersection = keras.sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "\n",
    "def unet_binary(pretrained_weights=None, input_size=(256, 256, 1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    normalize1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(normalize1)\n",
    "    #池化后做attention\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    normalize2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(normalize2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    normalize3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(normalize3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    normalize4 = BatchNormalization()(conv4)\n",
    "    #drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(normalize4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    \n",
    "    #drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv5))\n",
    "    normalize6 = BatchNormalization()(up6)\n",
    "    merge6 = concatenate([normalize4, normalize6], axis=3)\n",
    "    \n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    \n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv6))\n",
    "    normalize7 = BatchNormalization()(up7)\n",
    "    merge7 = concatenate([normalize3, normalize7], axis=3)\n",
    "    \n",
    "    #merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv7))\n",
    "    normalize8 = BatchNormalization()(up8)\n",
    "    \n",
    "    \n",
    "    merge8 = concatenate([normalize2, normalize8], axis=3)\n",
    "    \n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv8))\n",
    "    normalize9 = BatchNormalization()(up9)\n",
    "    \n",
    "    merge9 = concatenate([normalize1, normalize9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Dropout(0.2)(conv9)#, training=True)\n",
    "    \n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)  # Sigmoid activation for binary output\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    \n",
    "    model.compile(optimizer=Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-8), \n",
    "              loss=dice_loss, \n",
    "              metrics=[dice_coefficient, iou])\n",
    "\n",
    "    #model.compile(optimizer=Adam(learning_rate=1e-5), loss=dice_loss, metrics=['accuracy'])\n",
    "\n",
    "    if pretrained_weights:\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0b53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from __future__ import print_function\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # 对数变换\n",
    "    image = np.log1p(image)\n",
    "    # 归一化到 [0, 1]\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    return image\n",
    "\n",
    "        \n",
    "def split_train_val(image_folder, mask_folder, val_size):\n",
    "    image_paths = glob.glob(os.path.join(image_folder, '*.tif'))  # 假设使用png格式，根据实际情况修改\n",
    "    mask_paths = glob.glob(os.path.join(mask_folder, '*.png'))  # 假设掩模也是png格式\n",
    "\n",
    "    # 确保图像和掩模是匹配的\n",
    "    image_paths.sort()\n",
    "    mask_paths.sort()\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        image_paths, mask_paths, test_size=val_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_images, train_masks, val_images, val_masks\n",
    "\n",
    "def create_datagen(image_paths, mask_paths, batch_size, target_size):\n",
    "    # 将图像和掩码路径列表转换为 pandas DataFrame\n",
    "    image_df = pd.DataFrame({'filename': image_paths})\n",
    "    mask_df = pd.DataFrame({'filename': mask_paths})\n",
    "\n",
    "    image_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "    mask_datagen = ImageDataGenerator()\n",
    "\n",
    "    # 使用 DataFrame 代替列表\n",
    "    image_generator = image_datagen.flow_from_dataframe(\n",
    "        dataframe=image_df,\n",
    "        x_col='filename',\n",
    "        y_col=None,\n",
    "        class_mode=None,\n",
    "        color_mode='grayscale',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_dataframe(\n",
    "        dataframe=mask_df,\n",
    "        x_col='filename',\n",
    "        y_col=None,\n",
    "        class_mode=None,\n",
    "        color_mode='grayscale',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        img = next(image_generator)\n",
    "        mask = next(mask_generator)\n",
    "        yield img, mask\n",
    "\n",
    "# 使用示例\n",
    "batch_size = 2\n",
    "target_size = (256, 256)\n",
    "\n",
    "# 调整为实际图像和掩模的路径\n",
    "image_folder = 'data/filter/image/band_1'\n",
    "mask_folder = 'data/filter/labels'\n",
    "\n",
    "# 现在可以使用train_gen和val_gen进行模型训练和验证了\n",
    "# 假设 train_images 和 val_images 是通过之前的分割得到的训练和验证图像路径列表\n",
    "train_images, train_masks, val_images, val_masks = split_train_val(image_folder, mask_folder, val_size=0.2)\n",
    "\n",
    "# 计算 steps_per_epoch 和 validation_steps\n",
    "steps_per_epoch = np.ceil(len(train_images) / batch_size)\n",
    "validation_steps = np.ceil(len(val_images) / batch_size)\n",
    "\n",
    "# 创建数据生成器\n",
    "train_gen = create_datagen(train_images, train_masks, batch_size, target_size)\n",
    "val_gen = create_datagen(val_images, val_masks, batch_size, target_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3331150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Adadelta进行自调节，取消外部回调实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fa6880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "160/160 [==============================] - 68s 419ms/step - loss: 0.5765 - dice_coefficient: 0.4239 - iou: 0.2538 - val_loss: 0.9044 - val_dice_coefficient: 0.0955 - val_iou: 0.0461\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90445, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.5487 - dice_coefficient: 0.4517 - iou: 0.2781 - val_loss: 0.6441 - val_dice_coefficient: 0.3559 - val_iou: 0.2234\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.90445 to 0.64406, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.5298 - dice_coefficient: 0.4706 - iou: 0.2937 - val_loss: 0.5567 - val_dice_coefficient: 0.4433 - val_iou: 0.2893\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64406 to 0.55668, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.5099 - dice_coefficient: 0.4905 - iou: 0.3098 - val_loss: 0.5172 - val_dice_coefficient: 0.4828 - val_iou: 0.3139\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55668 to 0.51720, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.4947 - dice_coefficient: 0.5057 - iou: 0.3238 - val_loss: 0.5103 - val_dice_coefficient: 0.4897 - val_iou: 0.3222\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51720 to 0.51030, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.4777 - dice_coefficient: 0.5227 - iou: 0.3376 - val_loss: 0.4994 - val_dice_coefficient: 0.5006 - val_iou: 0.3398\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51030 to 0.49936, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.4617 - dice_coefficient: 0.5387 - iou: 0.3525 - val_loss: 0.5019 - val_dice_coefficient: 0.4980 - val_iou: 0.3387\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49936\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.4490 - dice_coefficient: 0.5514 - iou: 0.3642 - val_loss: 0.4744 - val_dice_coefficient: 0.5256 - val_iou: 0.3557\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49936 to 0.47441, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.4323 - dice_coefficient: 0.5681 - iou: 0.3793 - val_loss: 0.4883 - val_dice_coefficient: 0.5117 - val_iou: 0.3544\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.47441\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.4176 - dice_coefficient: 0.5828 - iou: 0.3935 - val_loss: 0.4573 - val_dice_coefficient: 0.5427 - val_iou: 0.3832\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47441 to 0.45727, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.4039 - dice_coefficient: 0.5965 - iou: 0.4066 - val_loss: 0.4672 - val_dice_coefficient: 0.5327 - val_iou: 0.3723\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.45727\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.3895 - dice_coefficient: 0.6108 - iou: 0.4192 - val_loss: 0.4498 - val_dice_coefficient: 0.5502 - val_iou: 0.3867\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.45727 to 0.44978, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.3761 - dice_coefficient: 0.6242 - iou: 0.4318 - val_loss: 0.4441 - val_dice_coefficient: 0.5559 - val_iou: 0.3978\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.44978 to 0.44406, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 68s 422ms/step - loss: 0.3607 - dice_coefficient: 0.6397 - iou: 0.4474 - val_loss: 0.4608 - val_dice_coefficient: 0.5392 - val_iou: 0.3829\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44406\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 67s 421ms/step - loss: 0.3490 - dice_coefficient: 0.6513 - iou: 0.4572 - val_loss: 0.4599 - val_dice_coefficient: 0.5401 - val_iou: 0.3804\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44406\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.3330 - dice_coefficient: 0.6673 - iou: 0.4738 - val_loss: 0.4330 - val_dice_coefficient: 0.5669 - val_iou: 0.4135\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.44406 to 0.43304, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 17/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.3240 - dice_coefficient: 0.6763 - iou: 0.4846 - val_loss: 0.4032 - val_dice_coefficient: 0.5968 - val_iou: 0.4351\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.43304 to 0.40323, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 18/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.3193 - dice_coefficient: 0.6810 - iou: 0.4904 - val_loss: 0.4056 - val_dice_coefficient: 0.5944 - val_iou: 0.4383\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.40323\n",
      "Epoch 19/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.2997 - dice_coefficient: 0.7006 - iou: 0.5086 - val_loss: 0.3898 - val_dice_coefficient: 0.6102 - val_iou: 0.4563\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.40323 to 0.38976, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 20/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.2904 - dice_coefficient: 0.7099 - iou: 0.5194 - val_loss: 0.3701 - val_dice_coefficient: 0.6299 - val_iou: 0.4759\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.38976 to 0.37011, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 21/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.2848 - dice_coefficient: 0.7154 - iou: 0.5270 - val_loss: 0.4530 - val_dice_coefficient: 0.5470 - val_iou: 0.3993\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.37011\n",
      "Epoch 22/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.2758 - dice_coefficient: 0.7244 - iou: 0.5360 - val_loss: 0.4025 - val_dice_coefficient: 0.5975 - val_iou: 0.4480\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.37011\n",
      "Epoch 23/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.2661 - dice_coefficient: 0.7341 - iou: 0.5475 - val_loss: 0.3754 - val_dice_coefficient: 0.6246 - val_iou: 0.4765\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.37011\n",
      "Epoch 24/50\n",
      "160/160 [==============================] - 67s 419ms/step - loss: 0.2578 - dice_coefficient: 0.7425 - iou: 0.5574 - val_loss: 0.3833 - val_dice_coefficient: 0.6167 - val_iou: 0.4675\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.37011\n",
      "Epoch 25/50\n",
      "160/160 [==============================] - 68s 423ms/step - loss: 0.2481 - dice_coefficient: 0.7521 - iou: 0.5692 - val_loss: 0.3476 - val_dice_coefficient: 0.6524 - val_iou: 0.5041\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.37011 to 0.34756, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 26/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.2423 - dice_coefficient: 0.7579 - iou: 0.5751 - val_loss: 0.3978 - val_dice_coefficient: 0.6022 - val_iou: 0.4390\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.34756\n",
      "Epoch 27/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.2409 - dice_coefficient: 0.7594 - iou: 0.5778 - val_loss: 0.3979 - val_dice_coefficient: 0.6021 - val_iou: 0.4513\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.34756\n",
      "Epoch 28/50\n",
      "160/160 [==============================] - 67s 420ms/step - loss: 0.2334 - dice_coefficient: 0.7668 - iou: 0.5855 - val_loss: 0.3388 - val_dice_coefficient: 0.6612 - val_iou: 0.5148\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.34756 to 0.33879, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 29/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.2249 - dice_coefficient: 0.7753 - iou: 0.5970 - val_loss: 0.3772 - val_dice_coefficient: 0.6228 - val_iou: 0.4718\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.33879\n",
      "Epoch 30/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.2197 - dice_coefficient: 0.7805 - iou: 0.6035 - val_loss: 0.3349 - val_dice_coefficient: 0.6651 - val_iou: 0.5153\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.33879 to 0.33485, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 67s 421ms/step - loss: 0.2147 - dice_coefficient: 0.7855 - iou: 0.6097 - val_loss: 0.3536 - val_dice_coefficient: 0.6464 - val_iou: 0.5079\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.33485\n",
      "Epoch 32/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.2102 - dice_coefficient: 0.7901 - iou: 0.6156 - val_loss: 0.3714 - val_dice_coefficient: 0.6286 - val_iou: 0.4753\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.33485\n",
      "Epoch 33/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.2042 - dice_coefficient: 0.7960 - iou: 0.6234 - val_loss: 0.3546 - val_dice_coefficient: 0.6454 - val_iou: 0.4990\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.33485\n",
      "Epoch 34/50\n",
      "160/160 [==============================] - 67s 419ms/step - loss: 0.1978 - dice_coefficient: 0.8024 - iou: 0.6318 - val_loss: 0.3748 - val_dice_coefficient: 0.6252 - val_iou: 0.4694\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.33485\n",
      "Epoch 35/50\n",
      "160/160 [==============================] - 67s 419ms/step - loss: 0.1992 - dice_coefficient: 0.8010 - iou: 0.6304 - val_loss: 0.3078 - val_dice_coefficient: 0.6922 - val_iou: 0.5475\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.33485 to 0.30777, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 36/50\n",
      "160/160 [==============================] - 67s 419ms/step - loss: 0.1930 - dice_coefficient: 0.8072 - iou: 0.6384 - val_loss: 0.3259 - val_dice_coefficient: 0.6741 - val_iou: 0.5305\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.30777\n",
      "Epoch 37/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.1884 - dice_coefficient: 0.8118 - iou: 0.6448 - val_loss: 0.3308 - val_dice_coefficient: 0.6692 - val_iou: 0.5114\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.30777\n",
      "Epoch 38/50\n",
      "160/160 [==============================] - 67s 420ms/step - loss: 0.1823 - dice_coefficient: 0.8179 - iou: 0.6530 - val_loss: 0.3050 - val_dice_coefficient: 0.6950 - val_iou: 0.5456\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.30777 to 0.30501, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 39/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.1827 - dice_coefficient: 0.8175 - iou: 0.6516 - val_loss: 0.3152 - val_dice_coefficient: 0.6848 - val_iou: 0.5409\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.30501\n",
      "Epoch 40/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.1776 - dice_coefficient: 0.8226 - iou: 0.6598 - val_loss: 0.3138 - val_dice_coefficient: 0.6862 - val_iou: 0.5325\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.30501\n",
      "Epoch 41/50\n",
      "160/160 [==============================] - 67s 421ms/step - loss: 0.1760 - dice_coefficient: 0.8242 - iou: 0.6617 - val_loss: 0.3304 - val_dice_coefficient: 0.6696 - val_iou: 0.5251\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.30501\n",
      "Epoch 42/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.1725 - dice_coefficient: 0.8277 - iou: 0.6668 - val_loss: 0.3676 - val_dice_coefficient: 0.6324 - val_iou: 0.4815\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.30501\n",
      "Epoch 43/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.1697 - dice_coefficient: 0.8305 - iou: 0.6698 - val_loss: 0.3170 - val_dice_coefficient: 0.6830 - val_iou: 0.5294\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.30501\n",
      "Epoch 44/50\n",
      "160/160 [==============================] - 67s 419ms/step - loss: 0.1668 - dice_coefficient: 0.8334 - iou: 0.6748 - val_loss: 0.2894 - val_dice_coefficient: 0.7106 - val_iou: 0.5719\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.30501 to 0.28943, saving model to unet_band1_val_best.hdf5\n",
      "Epoch 45/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.1658 - dice_coefficient: 0.8345 - iou: 0.6761 - val_loss: 0.3586 - val_dice_coefficient: 0.6414 - val_iou: 0.4699\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.28943\n",
      "Epoch 46/50\n",
      "160/160 [==============================] - 67s 417ms/step - loss: 0.1627 - dice_coefficient: 0.8375 - iou: 0.6808 - val_loss: 0.3198 - val_dice_coefficient: 0.6801 - val_iou: 0.5277\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.28943\n",
      "Epoch 47/50\n",
      "160/160 [==============================] - 67s 420ms/step - loss: 0.1615 - dice_coefficient: 0.8387 - iou: 0.6825 - val_loss: 0.3048 - val_dice_coefficient: 0.6952 - val_iou: 0.5419\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.28943\n",
      "Epoch 48/50\n",
      "160/160 [==============================] - 68s 422ms/step - loss: 0.1594 - dice_coefficient: 0.8407 - iou: 0.6857 - val_loss: 0.3120 - val_dice_coefficient: 0.6879 - val_iou: 0.5338\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.28943\n",
      "Epoch 49/50\n",
      "160/160 [==============================] - 67s 421ms/step - loss: 0.1584 - dice_coefficient: 0.8418 - iou: 0.6867 - val_loss: 0.3336 - val_dice_coefficient: 0.6664 - val_iou: 0.5112\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.28943\n",
      "Epoch 50/50\n",
      "160/160 [==============================] - 67s 418ms/step - loss: 0.1572 - dice_coefficient: 0.8430 - iou: 0.6888 - val_loss: 0.2918 - val_dice_coefficient: 0.7082 - val_iou: 0.5584\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.28943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163a8a75370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "model = unet_binary()\n",
    "model_checkpoint = ModelCheckpoint('unet_band1_val_best.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n",
    "\n",
    "#model_checkpoint = ModelCheckpoint('best_model.hdf5', monitor='val_dice_coefficient', verbose=1, save_best_only=True)\n",
    "\n",
    "# 创建 ReduceLROnPlateau 回调实例\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "# 模型训练\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,  # 使用计算得到的值\n",
    "    epochs=50,\n",
    "    callbacks=[model_checkpoint],\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=validation_steps  # 使用计算得到的值\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106c593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6446e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8d1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cb468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cab89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238593cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75c356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf26",
   "language": "python",
   "name": "tf26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
